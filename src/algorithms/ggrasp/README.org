* Generative Grasping (ggrasp)

This repo contains two models:

** Generative Grasping CNN (GGCNN)

/Original README at [[https://github.com/dougsm/ggcnn]]/

GG-CNN is a lightweight, fully-convolutional network which predicts the quality
and pose of antipodal grasps at every pixel in an input depth image. The
lightweight and single-pass generative nature of GG-CNN allows for fast
execution and closed-loop control, enabling accurate grasping in dynamic
environments when objects are moved during the grasp attempt.

** GR-ConvNet

/Original README at https://github.com/skumra/robotic-grasping/

GR-ConvNet is a novel generative residual convolutional neural network based
model architecture which detects objects in the cameraâ€™s field of view and
predicts a suitable antipodal grasp configuration for the objects in the image.

* Obtaining the Training Data

GG-CNN can be trained on the Cornell Grasping dataset, and the Jacquard dataset.

* Training the Models

To train the models, we use guild.ai. First, navigate to the ~src~ folder (which contains the guild.yml).

Then, you see a list of operations. For example, to train the ggcnn2 model:

#+BEGIN_SRC bash
guild run ggcnn2:train loss:smooth_l1_loss smoothness_weight:0.5 dataset:JacquardDataset dataset-path:/path/to/dataset
#+END_SRC

Sample trained models can be found in the ~saved_models~ folder.
